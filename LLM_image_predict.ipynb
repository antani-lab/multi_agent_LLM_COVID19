{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65674919-0e1b-4f94-80d0-f966aca6810c",
   "metadata": {},
   "source": [
    "Code to implement the multi-modal LLM (GPT-4o) with reasoning model (GPT-o1) for CXR analysis\n",
    "This notebook is for method demonstration. If you want to replicate the experiment, please contact the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03c0e9-71b9-459c-b6ff-73197f970756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment the following lines to install the required packages\n",
    "\n",
    "# ! pip install --upgrade pip\n",
    "# ! pip install -q openai\n",
    "# ! pip install -q langchain\n",
    "# ! pip install -q transformers\n",
    "# ! pip install torch\n",
    "# ! pip install -q trainer\n",
    "# ! pip install -q open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24497903-e41b-445f-a578-8f7095dd5a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9aea1c-fa85-41de-9d92-ed9309447826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:13:52.538979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-10 15:13:52.554616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-10 15:13:52.559576: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-10 15:13:52.571460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\n",
    "from transformers import Trainer\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import requests\n",
    "import argparse\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4849b48-88b9-4499-b1c1-948229127986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = \"***\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8999c9-6606-4495-9ad5-12c4cab106ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the encoder for text and encoder for gpt image processing\n",
    "\n",
    "def encoder(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "encoded_dimention = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60f10e-8c32-42f2-8ae5-066fd1aeb972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop image: ['/root/GAN_models/covid_cv/crop_fold1/train/positive/1.2.826.0.1.3680043.10.474.419639.298706138782242084784855945802.png', '/root/GAN_models/covid_cv/crop_fold1/train/positive/chest_XR_205.png', '/root/GAN_models/covid_cv/crop_fold1/train/positive/1.2.826.0.1.3680043.10.474.419639.198791789717449124503990981583.png', '/root/GAN_models/covid_cv/crop_fold1/train/positive/chest_XR_1220.png', '/root/GAN_models/covid_cv/crop_fold1/train/positive/1.2.826.0.1.3680043.10.474.419639.692619205424417339465290726479.png']\n",
      "whole image: ['/root/GAN_models/covid_cv/fold1/train/positive/1.2.826.0.1.3680043.10.474.419639.298706138782242084784855945802.png', '/root/GAN_models/covid_cv/fold1/train/positive/chest_XR_205.png', '/root/GAN_models/covid_cv/fold1/train/positive/1.2.826.0.1.3680043.10.474.419639.198791789717449124503990981583.png', '/root/GAN_models/covid_cv/fold1/train/positive/chest_XR_1220.png', '/root/GAN_models/covid_cv/fold1/train/positive/1.2.826.0.1.3680043.10.474.419639.692619205424417339465290726479.png']\n"
     ]
    }
   ],
   "source": [
    "# select the images to be processed\n",
    "\n",
    "# the file list file is not provided, but it should contain the paths to the images and their corresponding crop paths\n",
    "# the file should have a column 'fold' to indicate the fold number, 'crop_path' for cropped image paths, and 'whole_path' for whole image paths\n",
    "# to access the image files, please conatct the author of the code or the dataset provider\n",
    "data = pd.read_csv('file_list.csv')\n",
    "val_data = data[data['fold'] == 1]\n",
    "val_cropimages = val_data['crop_path'].tolist()\n",
    "val_images = val_data['whole_path'].tolist()\n",
    "\n",
    "cropimg_paths = val_cropimages[:5]\n",
    "img_paths = val_images[:5]\n",
    "\n",
    "print(f\"crop image: {cropimg_paths}\")\n",
    "print(f\"whole image: {img_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ed0caf-f5aa-4d85-931b-29ad7d8fd985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define helper function for GPT-4o model\n",
    "\n",
    "def get_gpt_image_info(img_path, prompts, key, model):\n",
    "    \n",
    "    base64_image = encode_image(img_path)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {key}\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    role_instruct = \"\"\"\n",
    "    You are a radiologist responsible for reviewing the given chest X-ray and provide discription for it. \\\n",
    "    Your answer should cover the following contents. \\\n",
    "    Does the image present COVID-19 pneumonia ? \\\n",
    "    what is the mRALE(modified Radiographic Assessment of Lung Edema) score presented on the image ?\\\n",
    "    Describe the clinical findings ? \\\n",
    "    rate the pneumonia severity in one of the four levels: low, mild, moderate, and severe. \\\n",
    "    \"\"\"\n",
    "    \n",
    "    # prompt = \"Please retrieve the information from the input image.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", 'content': role_instruct},\n",
    "        {\"role\": \"user\", 'content': [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\":{\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}],\n",
    "        \"max_tokens\": 200},\n",
    "    ]\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.3,\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        # Extract the reply text\n",
    "        reply = response_json['choices'][0]['message']['content']\n",
    "        return reply\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51df9e7b-089b-4094-a313-b071bba331d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract key info from the GPT-4o response\n",
    "\n",
    "def extract_label(prompt, client, model='gpt-4o-mini'):\n",
    "    role_instruct = \"\"\"\n",
    "    You are a medical AI expert. Your task is to extract the key information from the response of a fine-tuned gpt-4o model.\\\n",
    "    Your final answer includes: \\\n",
    "    label: positive or negative \\\n",
    "    level: pneumonia severity from low, mild, moderate, severe \\\n",
    "    score: the mRALE score for lung edema from 0 to 24 \\\n",
    "    return the final response in a dict format.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", 'content': role_instruct},\n",
    "        {\"role\": \"user\", 'content': f\"response from the fine-tuned model: {prompt}\"},\n",
    "        ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3, # this is the degree of randomness of the model's output, 0 - 1.0\n",
    "    )\n",
    "    json_string = response.choices[0].message.content\n",
    "    # Convert the string to a dictionary\n",
    "    response_dict = json.loads(json_string)\n",
    "    return response_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8653074-147c-4775-9916-bcacf55791a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded to GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load the dinov2 regressor\n",
    "\n",
    "model_name = \"facebook/dinov2-base\"\n",
    "# the checkpoint of the trained regressor is saved to the regressor_model.zip, please unzip it (multiple files) to the checkpoint_dir\n",
    "checkpoint_dir = 'path/to/your/checkpoint'  # replace with your checkpoint path\n",
    "dinov2_feature_extractor = AutoImageProcessor.from_pretrained(model_name)\n",
    "regressor = AutoModelForImageClassification.from_pretrained(checkpoint_dir, num_labels=1)\n",
    "regressor.to(device)\n",
    "regressor.eval()\n",
    "print(\"model loaded to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f42a1-9978-4ba5-bf45-5daf7f0fb270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded to GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/models/mobilevit/feature_extraction_mobilevit.py:28: FutureWarning: The class MobileViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use MobileViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the mobile ViT classifier\n",
    "model_name = \"apple/mobilevit-small\"\n",
    "# the checkpoint of the trained classifier is saved to the classifier_model.zip, please unzip it (multiple files) to the checkpoint_dir\n",
    "checkpoint_dir = 'path/to/your/classifier_checkpoint'  # replace with your checkpoint path\n",
    "classifier = MobileViTForImageClassification.from_pretrained(checkpoint_dir)\n",
    "classifier.eval()\n",
    "classifier = classifier.to(device)\n",
    "mobilevit_processor = MobileViTFeatureExtractor.from_pretrained(model_name)\n",
    "print(\"model loaded to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a0ae38-e5ef-4312-a744-4da2f5968fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to convert the prediction by the classifier and regressor to text\n",
    "# LLM can only process text; using numbers as input causes errors\n",
    "\n",
    "def parse_classify_text(classify_result):\n",
    "    predict_label, prob_dict = classify_result\n",
    "    message = f\"The image is predicted as COVID-19 penumonia {predict_label}. The probability of being negative is {prob_dict['negative']:.3f}, and the probability of being positive is {prob_dict['positive']:.3f}.\"\n",
    "    return message\n",
    "\n",
    "def parse_regression(pred_score):\n",
    "    message = f\"The image is predicted to have an mRALE score of {pred_score}.\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e4bd7c1-812a-4a73-87e2-9d00e31ad8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to use classifier and regressor\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def classify_image(image_path, classifier):\n",
    "    # Preprocess the image\n",
    "    class_dict = {0:'negative', 1:'positive'}\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    inputs = mobilevit_processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = classifier(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    if not predicted_class in [0, 1]:\n",
    "        predicted_class = 0\n",
    "    class_probabilities = {class_dict[i]: probabilities[0, i].item() for i in range(len(class_dict))}\n",
    "    return class_dict[predicted_class], class_probabilities\n",
    "\n",
    "def predict_score(image_path, regressor):\n",
    "    # Preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    inputs = dinov2_feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = regressor(**inputs)\n",
    "    score = outputs.logits.item()\n",
    "    if score < 0:\n",
    "        score = 0\n",
    "    return round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c50c190-f75b-4ced-93f4-56e9c909ee93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the Biomed CLIP model\n",
    "# load the Biomed CLIP - PubMedBERT model\n",
    "# helper functions to prompt the Biomed CLIP for prediction\n",
    "\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
    "\n",
    "biomed_model, biomed_preprocessor = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "biomed_tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "\n",
    "\n",
    "def pubmed_predict(model, preprocessor, tokenizer, image_path, label_list):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    file_name = os.path.basename(image_path)\n",
    "    context_length = 256\n",
    "    template = 'this is a photo of '\n",
    "    image = preprocessor(Image.open(image_path)).to(device)\n",
    "    texts = tokenizer([template + l for l in label_list], context_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features, text_features, logit_scale = model(image.unsqueeze(0), texts)\n",
    "        logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "        sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n",
    "        logits = logits.cpu().numpy()\n",
    "        sorted_indices = sorted_indices.cpu().numpy()\n",
    "        # remove the batch dim\n",
    "        return file_name, sorted_indices[0], logits[0]\n",
    "    \n",
    "\n",
    "def parse_pubmed_result(file_name, sorted_indices, logits, label_list, threshold=0.1):\n",
    "    info_dict = {}\n",
    "    info_dict['filename'] = file_name\n",
    "    info_dict['text'] = []\n",
    "    info_dict['prob'] = []\n",
    "    for idx in range(len(label_list)):\n",
    "        if logits[sorted_indices[idx]] >= threshold:\n",
    "            info_dict['text'].append(label_list[sorted_indices[idx]])\n",
    "            info_dict['prob'].append(logits[sorted_indices[idx]])\n",
    "        else:\n",
    "            break\n",
    "    return info_dict\n",
    "\n",
    "# helper function for BioMed CLIP\n",
    "\n",
    "def parse_pubmed_result(file_name, sorted_indices, logits, label_list, threshold=0.1):\n",
    "    info_dict = {}\n",
    "    info_dict['filename'] = file_name\n",
    "    info_dict['text'] = []\n",
    "    info_dict['prob'] = []\n",
    "    for idx in range(len(label_list)):\n",
    "        if logits[sorted_indices[idx]] >= threshold:\n",
    "            info_dict['text'].append(label_list[sorted_indices[idx]])\n",
    "            info_dict['prob'].append(logits[sorted_indices[idx]])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    filename = info_dict['filename']\n",
    "    text_list = info_dict['text']\n",
    "    score_list = info_dict['prob']\n",
    "    output_text = \"image information retrieved by Microsoft BiomedCLIP-PubMedBERT model: \\n\"\n",
    "    for info_text, score in zip(text_list, score_list):\n",
    "        pred = f\"{info_text}: probability: {score:.3f} \\n\"\n",
    "        output_text += pred\n",
    "    return info_dict, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa60a95-148a-44ce-91c9-369500cfc9e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the fine-tuned GPT-4o text-to-image model\n",
    "# use different model IDs for different folds of cross-validation.\n",
    "# The model is fine-tuned on the COVID-19 pneumonia dataset, and it can be used to describe the image based on the system instruction.\n",
    "# The fine-tuned model is not available in the public domain, so you need to use your own model ID.\n",
    "\n",
    "prompt = \"describe the image based on the system instruction.\"\n",
    "model_id = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bde1f44f-1596-4d7f-a0bc-39c91b152e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is predicted as COVID-19 penumonia positive. The probability of being negative is 0.000, and the probability of being positive is 0.914.\n",
      "The image is predicted to have an mRALE score of 11.75.\n"
     ]
    }
   ],
   "source": [
    "# classify and predict mRALE score\n",
    "# need to convert numbers to text for downstream LLM processing\n",
    "\n",
    "classify_prediction = classify_image(img_paths[1], classifier)\n",
    "mobilevit_prompt = parse_classify_text(classify_prediction)\n",
    "\n",
    "print(mobilevit_prompt)\n",
    "\n",
    "pred_score = predict_score(cropimg_paths[1], regressor)\n",
    "dino_prompt = parse_regression(pred_score)\n",
    "\n",
    "print(dino_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f91f843d-ab6b-47f5-bf4a-23eac58c353c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image presents COVID-19 pneumonia with a mRALE score of 2. There is mild lung involvement with limited ground-glass opacities, affecting a small area of the lung. The pneumonia severity is mild.\n"
     ]
    }
   ],
   "source": [
    "# call fine-tuned GPT-4o for image-to-text\n",
    "\n",
    "gpt_prompt = get_gpt_image_info(img_paths[1], prompt, api_key, model_id)\n",
    "print(gpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be72f2e2-ff65-45eb-87a6-1c4ead9dfe06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'level': 'mild', 'score': 2}\n"
     ]
    }
   ],
   "source": [
    "# format the key info from the GPT-4o response\n",
    "\n",
    "gpt_format_answer = extract_label(gpt_prompt, client)\n",
    "print(gpt_format_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67866603-ce08-4ccb-8aa6-634022bb159b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the input key words list to be detected by BiomedCLIP - PubmedBERT\n",
    "\n",
    "labels = [\n",
    "    'chest X-ray',\n",
    "    'chest CT',\n",
    "    'chest MRI',\n",
    "    'Centred Rotated Inspiration Adequate Expiratory image',\n",
    "    'Centred Rotated Inspiration Hyperinflated',\n",
    "    'TRACHEA Normal in position',\n",
    "    'TRACHEA Shifted towards right',\n",
    "    'TRACHEA Shifted towards left.',\n",
    "    'MEDIASTINUM Normal outlines',\n",
    "    'MEDIASTINUM Widened mediastinal shadow',\n",
    "    'MEDIASTINUM Widened with lobulated outline',\n",
    "    'AORTA Tortuous aorta',\n",
    "    'AORTA Mural calcifications in aorta',\n",
    "    'AORTA Tortuous aorta with mural calcifications',\n",
    "    'HEART',\n",
    "    'HEART Normal in transverse diameter',\n",
    "    'HEART Enlarged in transverse diameter',\n",
    "    'HEART Reduced in transverse diameter',\n",
    "    'HEART almost tubular in shape',\n",
    "    'HEART Cardiac diameter may not be represented accurately',\n",
    "    'HILA LUNG FIELDS Normal pulmonary vasculature',\n",
    "    'HILA LUNG FIELDS Oligaemic',\n",
    "    'HILA LUNG FIELDS Plethoric',\n",
    "    'HILA Bronchovascular markings prominent',\n",
    "    'HILA Bronchovascular markings more prominent in upper lung fields — upper lobe diversion',\n",
    "    'HILA Bronchovascular No focal or diffuse parenchymal lesion',\n",
    "    'DIAPHRAGM Normal position & contour',\n",
    "    'DIAPHRAGM Elevated on right side',\n",
    "    'DIAPHRAGM Elevated on left side',\n",
    "    'DIAPHRAGM Hump on right side',\n",
    "    'DIAPHRAGM Hump on left side',\n",
    "    'DIAPHRAGM Lowered on RL side',\n",
    "    'DIAPHRAGM Flattened on RL side',\n",
    "    'Costophrenic (CP) ANGLES Clear',\n",
    "    'Costophrenic (CP) ANGLES Obscured on right side',\n",
    "    'Costophrenic (CP) ANGLES Obscured on left side',\n",
    "    'Costophrenic (CP) ANGLES Obscured on both sides',\n",
    "    'BONES No fracture',\n",
    "    'BONES lytic',\n",
    "    'BONES sclerotic lesion',\n",
    "    'SOFT TISSUE No calcification',\n",
    "    'SOFT TISSUE obvious swelling',\n",
    "    'IMPRESSION No detectable finding Inflammatory changes',\n",
    "    'IMPRESSION Inflammatory & fibrotic changes',\n",
    "    'IMPRESSION Inflammatory & bronchiectatic changes',\n",
    "    'IMPRESSION Chronic inflammatory & fibrotic changes',\n",
    "    'Bilateral pulmonary emphysematous changes [does not include mild disease]',\n",
    "    'COVID-19',\n",
    "    'covid',\n",
    "    'Severe acute respiratory syndrome coronavirus 2',\n",
    "    'SARS‑CoV‑2',\n",
    "    'CXR CATEGORY Classic covid',\n",
    "    'CXR CATEGORY Probable covid',\n",
    "    'CXR CATEGORY Indeterminate for covid',\n",
    "    'CXR CATEGORY Non-covid pathology',\n",
    "    'CXR GRADING Mild',\n",
    "    'CXR GRADING Moderate',\n",
    "    'CXR GRADING Severe',\n",
    "    'COMPARISON Stable',\n",
    "    'COMPARISON Marginal improvement',\n",
    "    'COMPARISON Progression',\n",
    "    'COMPARISON Significant improvement',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af7ad624-ea8c-42d5-90ca-316732c4aa6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "005a9104-a2e8-4068-9fda-718274d4b6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "file_name, sorted_indices, logits = pubmed_predict(biomed_model, biomed_preprocessor, biomed_tokenizer, img_paths[1], labels)\n",
    "biomed_dict, biomed_prompt = parse_pubmed_result(file_name, sorted_indices, logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "590d5789-c841-4bfc-9db7-9eda71e5d046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'chest_XR_205.png', 'text': ['CXR GRADING Mild', 'CXR CATEGORY Non-covid pathology'], 'prob': [0.44146204, 0.19367959]}\n",
      "image information retrieved by Microsoft BiomedCLIP-PubMedBERT model: \n",
      "CXR GRADING Mild: probability: 0.441 \n",
      "CXR CATEGORY Non-covid pathology: probability: 0.194 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(biomed_dict)\n",
    "print(biomed_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a26983ef-b652-49a0-8349-53d27ab32216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT o1-mini\n",
    "# use this function to integrate all outputs into the GPT-o1 model for final reasoning\n",
    "\n",
    "def reasoning_predict(gpt_4o_prompt, gpt_format_answer, dino_prompt, vit_prompt, biomed_prompt, client, model='o1-mini'):\n",
    "    role_instruct = \"\"\"\n",
    "    You are a medical AI expert. Your task is to extract the key information from the predictions of multiple AI models including: \\\n",
    "    A fine-tuned Mobile ViT model to predict whether the chest x-ray image has COVID-19 pneumonia with prediction Accuracy of 0.86, F1 Score of 0.92, Sensitivity of 0.95 for positive, and Specificity of 0.54. \\\n",
    "    A fine-tuned Dinov2 model to predict the mRALE (Modified Radiographic Assessment of Lung Edema) score to evaluate the severity of the pneumonia as presented in the image. \\\n",
    "    If the mRALE is 0, the image should be classified as low for penumonia; if the mRALE is between 1 to 10, the image should be classified as mild penumonia; \\\n",
    "    if the mRALE is between 11 to 18, the image should be classified as moderate pnuemonia; if the mRALE is above 19, the image should be classified as severe penumonia; \\\n",
    "    The fine-tuned Dinov2 model has the prediction Mean Squared Error of 73.74, Mean Absolute Error of 6.85, Root Mean Squared Error of 8.59, R-Squared of -0.14, and Explained Variance Score of -0.13 for mRALE prediction. \\\n",
    "    A fine-tuned GPT-4o for image-to-text generation to predict chest x-ray images on the COVID-19 pneumonia and pneumonia severity. \\\n",
    "    For COVID-19 pneumonia prediction, the fine-tuned GPT-4o has Accuracy of 0.86, F1 Score of 0.92, Sensitivity of 0.97, and Specificity of 0.48. \\ \n",
    "    For mRALE score prediction, the fine-tuned GPT-4o model has the prediction Mean Squared Error of 31.82, Mean Absolute Error of 3.69, Root Mean Squared Error of 5.64, R-Squared of 0.51, and Explained Variance Score of 0.54 for mRALE prediction. \\\n",
    "    A pretrained BiomedCLIP-PubMedBERT model to retrieve the diagnostic entities from the image. Note that a probability score greater than 0.1 is considered significant. \\\n",
    "    Your final decision should not be based on the prediction by any single model exclusively. If the predictions by different models have big difference, consider assign proper weights to each prediction to finalize your answer. \\ \n",
    "    Your final answer must include the following content: \\\n",
    "    whether the image is COVID-19 pneumonia positive or COVID-19 pneumonia negative \\\n",
    "    The level of pneumonia severity from low, mild, moderate, or severe \\\n",
    "    pneumonia score: the mRALE score for lung edema from 0 to 24 \\\n",
    "    return the final response in the dict format with the key \"covid_19\" for the COVID-19 pneumonia, with the key \"severity\" for the pneumonia sererity level, \\\n",
    "    with the key \"mRALE\" for the numeric mRALE score, and explanation of the reasoning procedure respectively.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", 'content': role_instruct},\n",
    "        {\"role\": \"user\", 'content': f\"predicted information from the fine-tuned gpt-4o model: {gpt_4o_prompt}\"},\n",
    "        {\"role\": \"user\", 'content': f\"predictions by the fine-tuned gpt-4o model: covid pneumonia: {gpt_format_answer['label']}, severity: {gpt_format_answer['level']}, mRALE score: {gpt_format_answer['score']}\"},\n",
    "        {\"role\": \"user\", 'content': f\"predicted information from the fine-tuned Dinov2 model for mRALE score with mean absolut error (MAE) of 5 points: {dino_prompt}\"},\n",
    "        {\"role\": \"user\", 'content': f\"predicted information from the fine-tuned Mobile ViT model for COVID-19 pneumonia mean accuracy of 0.86: {vit_prompt}\"},\n",
    "        {\"role\": \"user\", 'content': f\"predicted information from the pretrained BiomedCLIP-PubMedBERT model to retrieve the diagnostic entities: {biomed_prompt}\"},\n",
    "        ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c9bfe92-5422-48ba-a339-2de4642f4743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"covid_19\": \"positive\",\n",
      "  \"severity\": \"mild\",\n",
      "  \"mRALE\": 6,\n",
      "  \"explanation\": \"After evaluating predictions from multiple models, the image is classified as COVID-19 pneumonia positive based on consistent outputs from both the Mobile ViT and GPT-4o models, which showed high probabilities for a positive diagnosis. For the mRALE score, the GPT-4o model predicted a score of 1 with a lower mean absolute error (MAE) of 3.69 compared to the Dinov2 model's prediction of 11.75 and MAE of 5. Given the lower error margin, greater weight was assigned to the GPT-4o prediction. Combining the scores with appropriate weighting results in an adjusted mRALE score of approximately 6, categorizing the pneumonia severity as mild.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "pred_text = reasoning_predict(gpt_prompt, gpt_format_answer, dino_prompt, mobilevit_prompt, biomed_prompt, client)\n",
    "# reason_pred = reason_text_format(pred_text)\n",
    "print(pred_text)\n",
    "# print(reason_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37e973-0d47-4c7a-b509-f98b15898aba",
   "metadata": {},
   "source": [
    "# pick the fold index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143439f-59e1-4979-9d5b-27090c2695b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "# load the file list for cross-validation\n",
    "# the file list file is not provided, but it should contain the paths to the images and their corresponding crop paths\n",
    "# the file should have a column 'fold' to indicate the fold number, 'crop_path' for cropped image paths, and 'whole_path' for whole image paths\n",
    "# to access the image files, please contact the author of the code or the dataset provider\n",
    "file_data = pd.read_csv('file_list.csv')\n",
    "val_data = file_data[file_data['fold'] == 5] # change the fold number to select the validation set\n",
    "val_cropimages = val_data['crop_path'].tolist()\n",
    "val_images = val_data['whole_path'].tolist()\n",
    "print(len(val_images))\n",
    "# cropimg_paths = val_cropimages[:2]\n",
    "# img_paths = val_images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a62e25d9-76da-46be-93c4-276815d37db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biomed_labels = [\n",
    "    'chest X-ray',\n",
    "    'chest CT',\n",
    "    'chest MRI',\n",
    "    'Centred Rotated Inspiration Adequate Expiratory image',\n",
    "    'Centred Rotated Inspiration Hyperinflated',\n",
    "    'TRACHEA Normal in position',\n",
    "    'TRACHEA Shifted towards right',\n",
    "    'TRACHEA Shifted towards left.',\n",
    "    'MEDIASTINUM Normal outlines',\n",
    "    'MEDIASTINUM Widened mediastinal shadow',\n",
    "    'MEDIASTINUM Widened with lobulated outline',\n",
    "    'AORTA Tortuous aorta',\n",
    "    'AORTA Mural calcifications in aorta',\n",
    "    'AORTA Tortuous aorta with mural calcifications',\n",
    "    'HEART',\n",
    "    'HEART Normal in transverse diameter',\n",
    "    'HEART Enlarged in transverse diameter',\n",
    "    'HEART Reduced in transverse diameter',\n",
    "    'HEART almost tubular in shape'\n",
    "    'HEART Cardiac diameter may not be represented accurately',\n",
    "    'HILA LUNG FIELDS Normal pulmonary vasculature',\n",
    "    'HILA LUNG FIELDS Oligaemic',\n",
    "    'HILA LUNG FIELDS Plethoric',\n",
    "    'HILA Bronchovascular markings prominent',\n",
    "    'HILA Bronchovascular markings more prominent in upper lung fields — upper lobe diversion',\n",
    "    'HILA Bronchovascular No focal or diffuse parenchymal lesion',\n",
    "    'DIAPHRAGM Normal position & contour',\n",
    "    'DIAPHRAGM Elevated on right side',\n",
    "    'DIAPHRAGM Elevated on left side',\n",
    "    'DIAPHRAGM Hump on right side',\n",
    "    'DIAPHRAGM Hump on left side',\n",
    "    'DIAPHRAGM Lowered on RL side',\n",
    "    'DIAPHRAGM Flattened on RL side',\n",
    "    'Costophrenic (CP) ANGLES Clear',\n",
    "    'Costophrenic (CP) ANGLES Obscured on right side',\n",
    "    'Costophrenic (CP) ANGLES Obscured on left side',\n",
    "    'Costophrenic (CP) ANGLES Obscured on both sides',\n",
    "    'BONES No fracture',\n",
    "    'BONES lytic',\n",
    "    'BONES sclerotic lesion',\n",
    "    'SOFT TISSUE No calcification',\n",
    "    'SOFT TISSUE obvious swelling',\n",
    "    'IMPRESSION No detectable finding Inflammatory changes',\n",
    "    'IMPRESSION Inflammatory & fibrotic changes',\n",
    "    'IMPRESSION Inflammatory & bronchiectatic changes',\n",
    "    'IMPRESSION Chronic inflammatory & fibrotic changes',\n",
    "    'Bilateral pulmonary emphysematous changes [does not include mild disease]',\n",
    "    'COVID-19',\n",
    "    'covid',\n",
    "    'Severe acute respiratory syndrome coronavirus 2',\n",
    "    'SARS‑CoV‑2',\n",
    "    'CXR CATEGORY Classic covid',\n",
    "    'CXR CATEGORY Probable covid',\n",
    "    'CXR CATEGORY Indeterminate for covid',\n",
    "    'CXR CATEGORY Non-covid pathology',\n",
    "    'CXR GRADING Mild',\n",
    "    'CXR GRADING Moderate',\n",
    "    'CXR GRADING Severe',\n",
    "    'COMPARISON Stable',\n",
    "    'COMPARISON Marginal improvement',\n",
    "    'COMPARISON Progression',\n",
    "    'COMPARISON Significant improvement',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c4a51ff-a56d-4aed-8f60-e7aa17c276b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions saved.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/root/GAN_models/covid_cv/reason_val.csv')\n",
    "data['pred_class'] = data['pred_class'].astype(str)\n",
    "data['gpt_class'] = data['gpt_class'].astype(str)\n",
    "data['gpt_text'] = data['gpt_text'].astype(str)\n",
    "data['gpt_class'] = data['gpt_class'].astype(str)\n",
    "data['gpt_level'] = data['gpt_level'].astype(str)\n",
    "data['biomed_text'] = data['biomed_text'].astype(str)\n",
    "data['reason_class'] = data['reason_class'].astype(str)\n",
    "data['reason_level'] = data['reason_level'].astype(str)\n",
    "data['explain'] = data['explain'].astype(str)\n",
    "data['reason_text'] = data['reason_text'].astype(str)\n",
    "\n",
    "\n",
    "for image_path, cropimage_path in zip(val_images, val_cropimages):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    row_index = data.index[data['filename'] == image_name].tolist()\n",
    "    if len(row_index) > 1:\n",
    "        print(\"more than one image is identified.\")\n",
    "    idx = row_index[0]\n",
    "\n",
    "    if not row_index:\n",
    "        print(f\"Image {image_name} cannot be found in the dataset.\")\n",
    "        continue\n",
    "    \n",
    "    # classify and predict mRALE score\n",
    "    classify_prediction = classify_image(image_path, classifier)\n",
    "    mobilevit_prompt = parse_classify_text(classify_prediction)\n",
    "    pred_score = predict_score(cropimage_path, regressor)\n",
    "    dino_prompt = parse_regression(pred_score)\n",
    "    gpt_prompt = \"describe the image based on the system instruction.\"\n",
    "    gpt_prompt = get_gpt_image_info(image_path, gpt_prompt, api_key, model_id)\n",
    "    format_answer = extract_label(gpt_prompt, client)\n",
    "    torch.cuda.empty_cache()\n",
    "    file_name, sorted_indices, logits = pubmed_predict(biomed_model, biomed_preprocessor, biomed_tokenizer, image_path, biomed_labels)\n",
    "    biomed_dict, biomed_prompt = parse_pubmed_result(file_name, sorted_indices, logits, biomed_labels)\n",
    "    pred_text = reasoning_predict(gpt_prompt, format_answer, dino_prompt, mobilevit_prompt, biomed_prompt, client)\n",
    "    # json_text = pred_text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    # json_dict = json.loads(json_text)\n",
    "\n",
    "\n",
    "    data.loc[idx, 'pred_class'] = classify_prediction[0]\n",
    "    data.loc[idx, 'pred_score'] = pred_score\n",
    "    data.loc[idx, 'gpt_text'] = gpt_prompt\n",
    "    data.loc[idx, 'gpt_class'] = format_answer['label']\n",
    "    data.loc[idx, 'gpt_score'] = format_answer['score']\n",
    "    data.loc[idx, 'gpt_level'] = format_answer['level']\n",
    "    data.loc[idx, 'biomed_text'] = biomed_prompt\n",
    "    data.loc[idx, 'reason_text'] = pred_text\n",
    "    # data.loc[idx, 'reason_class'] = json_dict['covid_19']\n",
    "    # data.loc[idx, 'reason_score'] = json_dict['mRALE']\n",
    "    # data.loc[idx, 'reason_level'] = json_dict['severity']\n",
    "    # data.loc[idx, 'explain'] = json_dict['explanation']\n",
    "    \n",
    "data.to_csv('/root/GAN_models/covid_cv/reason_val.csv', index=False)\n",
    "print(\"predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54470c39-041e-4dd7-9884-1bbff0d17328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# Replace '***' with your actual OpenAI API key\n",
    "api_key = \"***\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38133717-b270-44c1-a9d4-90c2860459df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file and ensure all columns are of string type\n",
    "# The CSV file should contain the predictions from the previous step\n",
    "# The file should have columns: 'pred_class', 'gpt_class', 'gpt_text', 'gpt_level', 'biomed_text', 'reason_class', 'reason_level', 'explain', 'reason_text', 'reason_score'\n",
    "# please contact the author of the code or the dataset provider to access the file\n",
    "data = pd.read_csv('reason_val.csv')\n",
    "data['pred_class'] = data['pred_class'].astype(str)\n",
    "data['gpt_class'] = data['gpt_class'].astype(str)\n",
    "data['gpt_text'] = data['gpt_text'].astype(str)\n",
    "data['gpt_class'] = data['gpt_class'].astype(str)\n",
    "data['gpt_level'] = data['gpt_level'].astype(str)\n",
    "data['biomed_text'] = data['biomed_text'].astype(str)\n",
    "data['reason_class'] = data['reason_class'].astype(str)\n",
    "data['reason_level'] = data['reason_level'].astype(str)\n",
    "data['explain'] = data['explain'].astype(str)\n",
    "data['reason_text'] = data['reason_text'].astype(str)\n",
    "data['reason_class'] = data['reason_class'].astype(str)\n",
    "data['reason_score'] = data['reason_score'].astype(str)\n",
    "data['reason_level'] = data['reason_level'].astype(str)\n",
    "data['explain'] = data['explain'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e8bce83-1bd8-4bb4-b41e-954eaf4d937d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract key info from the GPT-4o response\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_reason(prompt, client, model='gpt-4o-mini'):\n",
    "    role_instruct = \"\"\"\n",
    "    You are a medical AI expert. Your task is to extract the key information from the response from the gpt-o1 model.\\\n",
    "    Your final answer includes: \\\n",
    "    covid_19: positive or negative \\\n",
    "    severity: pneumonia severity from low, mild, moderate, severe \\\n",
    "    mRALE: the mRALE score for lung edema from 0 to 24 \\\n",
    "    explanation: the explanation text for how the decision is made. \\\n",
    "    Output a JSON object with the extracted information.\".\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", 'content': role_instruct},\n",
    "        {\"role\": \"user\", 'content': f\"response from the fine-tuned model: {prompt}\"},\n",
    "        ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3, # this is the degree of randomness of the model's output, 0 - 1.0\n",
    "    )\n",
    "    json_text = response.choices[0].message.content.strip()\n",
    "    # Convert the string to a dictionary\n",
    "    # data_dict = json.loads(json_text)\n",
    "    return json_text\n",
    "\n",
    "\n",
    "\n",
    "def format_reason(prompt):\n",
    "    json_text = prompt.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    data_dict = json.loads(json_text)\n",
    "    return data_dict['covid_19'], data_dict['severity'], data_dict['mRALE'], data_dict['explanation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8caa239-e24e-49d8-b465-314c17d0c3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive mild 1 Multiple models indicate a positive diagnosis for COVID-19 pneumonia with high confidence. The Mobile ViT and GPT-4o models both predict COVID-19 pneumonia as positive, supported by high sensitivity and strong probability scores for positivity. Regarding pneumonia severity, the GPT-4o model predicts an mRALE score of 1, which falls into the 'mild' category based on the provided classification criteria (mRALE 1-10). Although the Dinov2 model predicts a higher mRALE score of 12.04, its performance metrics (Mean Squared Error of 73.74, R-Squared of -0.14) suggest lower reliability. Additionally, the BiomedCLIP-PubMedBERT model's CXR grading further supports a mild severity classification with the highest probability assigned to 'mild'. Therefore, considering the overall consensus and reliability of each model's predictions, the final assessment is a positive COVID-19 pneumonia diagnosis with mild severity and an mRALE score of 1.\n"
     ]
    }
   ],
   "source": [
    "explain_list = data['reason_text'].tolist()\n",
    "prompt = explain_list[1]\n",
    "response = extract_reason(prompt, client)\n",
    "classify, level, score, explain = format_reason(response)\n",
    "print(classify, level, score, explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12534d93-70c6-45c7-bb82-cb0750d915d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing\n"
     ]
    }
   ],
   "source": [
    "data['reason_class'] = data['reason_class'].astype(str)\n",
    "data['reason_level'] = data['reason_level'].astype(str)\n",
    "data['explain'] = data['explain'].astype(str)\n",
    "data['reason_score'] = data['reason_score'].astype(float)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    input_text = row['reason_text']\n",
    "    extract_json = extract_reason(input_text, client)\n",
    "    classify, level, score, explain = format_reason(extract_json)\n",
    "    data.loc[index, 'reason_class'] = classify\n",
    "    data.loc[index, 'reason_score'] = score\n",
    "    data.loc[index, 'reason_level'] = level\n",
    "    data.loc[index, 'explain'] = explain\n",
    "    \n",
    "data.to_csv('/root/GAN_models/covid_cv/reason_val.csv', index=False)\n",
    "print(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c64724fe-f04a-46d4-86ba-65738253b645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>filename</th>\n",
       "      <th>extent_right</th>\n",
       "      <th>density_right</th>\n",
       "      <th>extent_left</th>\n",
       "      <th>density_left</th>\n",
       "      <th>extent_right_numerical</th>\n",
       "      <th>density_right_numerical</th>\n",
       "      <th>extent_left_numerical</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt_score</th>\n",
       "      <th>gpt_level</th>\n",
       "      <th>gpt_text</th>\n",
       "      <th>reason_class</th>\n",
       "      <th>reason_score</th>\n",
       "      <th>reason_level</th>\n",
       "      <th>reason_text</th>\n",
       "      <th>explain</th>\n",
       "      <th>whole_path</th>\n",
       "      <th>crop_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.2.826.0.1.3680043.10.474.419639.298706138782...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>low</td>\n",
       "      <td>This image presents COVID-19 pneumonia. The mR...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>mild</td>\n",
       "      <td>```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...</td>\n",
       "      <td>Multiple AI models were analyzed to determine ...</td>\n",
       "      <td>/root/GAN_models/covid_cv/fold1/train/positive...</td>\n",
       "      <td>/root/GAN_models/covid_cv/crop_fold1/train/pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>chest_XR_835.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>low</td>\n",
       "      <td>This image presents COVID-19 pneumonia with a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mild</td>\n",
       "      <td>```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...</td>\n",
       "      <td>Multiple models indicate a positive diagnosis ...</td>\n",
       "      <td>/root/GAN_models/covid_cv/fold1/train/positive...</td>\n",
       "      <td>/root/GAN_models/covid_cv/crop_fold1/train/pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.2.826.0.1.3680043.10.474.419639.270940636091...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>The image presents COVID-19 pneumonia with a m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...</td>\n",
       "      <td>The final assessment integrates predictions fr...</td>\n",
       "      <td>/root/GAN_models/covid_cv/fold1/train/positive...</td>\n",
       "      <td>/root/GAN_models/covid_cv/crop_fold1/train/pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.2.826.0.1.3680043.10.474.419639.246480861434...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mild</td>\n",
       "      <td>The image presents COVID-19 pneumonia with a m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mild</td>\n",
       "      <td>```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...</td>\n",
       "      <td>Both the fine-tuned GPT-4o and Mobile ViT mode...</td>\n",
       "      <td>/root/GAN_models/covid_cv/fold1/train/positive...</td>\n",
       "      <td>/root/GAN_models/covid_cv/crop_fold1/train/pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>chest_XR_611.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>This is a case of COVID-19 pneumonia with no l...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>mild</td>\n",
       "      <td>```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...</td>\n",
       "      <td>Multiple models indicate a positive diagnosis ...</td>\n",
       "      <td>/root/GAN_models/covid_cv/fold1/val/positive/c...</td>\n",
       "      <td>/root/GAN_models/covid_cv/crop_fold1/val/posit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     label                                           filename  \\\n",
       "0           0  positive  1.2.826.0.1.3680043.10.474.419639.298706138782...   \n",
       "1           1  positive                                   chest_XR_835.png   \n",
       "2           2  positive  1.2.826.0.1.3680043.10.474.419639.270940636091...   \n",
       "3           3  positive  1.2.826.0.1.3680043.10.474.419639.246480861434...   \n",
       "4           4  positive                                   chest_XR_611.png   \n",
       "\n",
       "  extent_right density_right extent_left density_left  extent_right_numerical  \\\n",
       "0          NaN           NaN         NaN          NaN                     0.0   \n",
       "1          NaN           NaN         NaN          NaN                     0.0   \n",
       "2          NaN           NaN         NaN          NaN                     0.0   \n",
       "3          NaN           NaN         NaN          NaN                     0.0   \n",
       "4          NaN           NaN         NaN          NaN                     0.0   \n",
       "\n",
       "   density_right_numerical  extent_left_numerical  ...  gpt_score  gpt_level  \\\n",
       "0                      0.0                    0.0  ...        1.0        low   \n",
       "1                      0.0                    0.0  ...        1.0        low   \n",
       "2                      0.0                    0.0  ...        0.0        low   \n",
       "3                      0.0                    0.0  ...        1.0       mild   \n",
       "4                      0.0                    0.0  ...        0.0        low   \n",
       "\n",
       "                                            gpt_text  reason_class  \\\n",
       "0  This image presents COVID-19 pneumonia. The mR...      positive   \n",
       "1  This image presents COVID-19 pneumonia with a ...      positive   \n",
       "2  The image presents COVID-19 pneumonia with a m...      positive   \n",
       "3  The image presents COVID-19 pneumonia with a m...      positive   \n",
       "4  This is a case of COVID-19 pneumonia with no l...      positive   \n",
       "\n",
       "   reason_score  reason_level  \\\n",
       "0           5.0          mild   \n",
       "1           1.0          mild   \n",
       "2           0.0           low   \n",
       "3           1.0          mild   \n",
       "4           4.0          mild   \n",
       "\n",
       "                                         reason_text  \\\n",
       "0  ```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...   \n",
       "1  ```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...   \n",
       "2  ```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...   \n",
       "3  ```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...   \n",
       "4  ```json\\n{\\n  \"covid_19\": \"positive\",\\n  \"seve...   \n",
       "\n",
       "                                             explain  \\\n",
       "0  Multiple AI models were analyzed to determine ...   \n",
       "1  Multiple models indicate a positive diagnosis ...   \n",
       "2  The final assessment integrates predictions fr...   \n",
       "3  Both the fine-tuned GPT-4o and Mobile ViT mode...   \n",
       "4  Multiple models indicate a positive diagnosis ...   \n",
       "\n",
       "                                          whole_path  \\\n",
       "0  /root/GAN_models/covid_cv/fold1/train/positive...   \n",
       "1  /root/GAN_models/covid_cv/fold1/train/positive...   \n",
       "2  /root/GAN_models/covid_cv/fold1/train/positive...   \n",
       "3  /root/GAN_models/covid_cv/fold1/train/positive...   \n",
       "4  /root/GAN_models/covid_cv/fold1/val/positive/c...   \n",
       "\n",
       "                                           crop_path  \n",
       "0  /root/GAN_models/covid_cv/crop_fold1/train/pos...  \n",
       "1  /root/GAN_models/covid_cv/crop_fold1/train/pos...  \n",
       "2  /root/GAN_models/covid_cv/crop_fold1/train/pos...  \n",
       "3  /root/GAN_models/covid_cv/crop_fold1/train/pos...  \n",
       "4  /root/GAN_models/covid_cv/crop_fold1/val/posit...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a258c878-799f-4958-8768-364bcf67c298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_text = extract_json.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "data_dict = json.loads(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "053d4c4d-935f-4df9-8f1b-2d1fcd4fc258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covid_19': 'positive',\n",
       " 'severity': 'moderate',\n",
       " 'mRALE': 11.8,\n",
       " 'explanation': 'The final assessment classifies the image as COVID-19 pneumonia positive with moderate severity based on a weighted average mRALE score of 11.8, which falls within the moderate severity range.'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f3c2168-a13b-46b5-bf4d-c49b0f32acac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_png_files(img_dir):\n",
    "    png_files = []\n",
    "    for subdir, _, files in os.walk(img_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                # Get the absolute path and add it to the list\n",
    "                png_files.append(os.path.abspath(os.path.join(subdir, file)))\n",
    "    return png_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98464aab-6b2e-421f-8f50-f44dec5cae68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dir = '/root/GAN_models/covid_cv/fold1/val'\n",
    "val_imgs = get_png_files(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b261d637-6e79-48ae-bd84-23b08153414b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "print(len(val_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc8d85-e941-4aa6-a68c-ecb6256f49cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6294/2953101021.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'positive' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_index, 'gpt_covid'] = format_answer['label']\n",
      "/tmp/ipykernel_6294/2953101021.py:30: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'severe' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_index, 'gpt_level'] = format_answer['level']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 21}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 21}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 1, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'moderate', 'level_score': 0.7, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 1, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 19}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 21}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 1, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 1, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'moderate', 'level_score': 0.8, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'moderate', 'level_score': 0.8, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 21}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 20}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'moderate', 'level_score': 0.8, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 1, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 1.0, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.9, 'score': 18}\n",
      "{'label': 'positive', 'label_score': 0.95, 'level': 'severe', 'level_score': 0.95, 'score': 18}\n",
      "Request failed with status code 500\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m     21\u001b[0m     response \u001b[38;5;241m=\u001b[39m get_image_info(img_path, prompt, api_key, model_id)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43manswer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n\u001b[1;32m     24\u001b[0m format_answer \u001b[38;5;241m=\u001b[39m extract_label(answer, client)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(format_answer)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# use this part to reconstruct the outputs into structured data for performance evaluation\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# load the validation set file\n",
    "# the file should have a column 'filename' for the image file names, and 'cvt-1' for the predictions by the BiomedCLIP model\n",
    "# please contact the author of the code or the dataset provider to access the file\n",
    "data = pd.read_csv('val_chatgpt.csv')\n",
    "\n",
    "# data['segformer-2'] = pd.NA\n",
    "# data['cvt-1'] = data['cvt-1'].astype(str)\n",
    "model_id = '****'  # replace with your fine-tuned model ID\n",
    "pred_labels = []\n",
    "for image_path in val_imgs:\n",
    "    image_name = os.path.basename(image_path)\n",
    "    row_index = data.index[data['filename'] == image_name].tolist()\n",
    "    if not row_index:\n",
    "        print(f'image {image_name} cannot be found')\n",
    "    prompt1 = \"given the image, detect if the image has covid-19 positive pneumonia or covid-19 negative with a confidence score from 0 to 1.\"\n",
    "    prompt2 = \"rate the pneumonia at one of the four level: low, mild, moderate, severe with a confidence score from 0 to 1.\"\n",
    "    prompt3 = \"predict the mRALE (from 0 to 24, zero means no pneumonia) score of the image\"\n",
    "    prompts = [prompt1, prompt2, prompt3]\n",
    "    answer = \"\"\n",
    "    for prompt in prompts:\n",
    "        response = get_image_info(img_path, prompt, api_key, model_id)\n",
    "        answer += response\n",
    "\n",
    "    format_answer = extract_label(answer, client)\n",
    "    print(format_answer)\n",
    "    \n",
    "    data.loc[row_index, 'gpt_covid'] = format_answer['label']\n",
    "    data.loc[row_index, 'covid_conf'] = format_answer['label_score']\n",
    "    data.loc[row_index, 'gpt_mrale'] = format_answer['score']\n",
    "    data.loc[row_index, 'gpt_level'] = format_answer['level']\n",
    "    data.loc[row_index, 'level_conf'] = format_answer['level_score']\n",
    "    \n",
    "data.to_csv('val_chatgpt.csv', index=False)\n",
    "print(\"predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6029b454-a002-48fd-b34d-edf64ae2d7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
